{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c663b4eb",
   "metadata": {},
   "source": [
    "# Spotify: Top 200 Weekly (Global) Song Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cd2028",
   "metadata": {},
   "source": [
    "This notebook provides an in-depth look at the hyper-parameters used for several different models when running a gridsearch. The models ran can be seen [here](./02_Preprocessing_And_Modeling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed29253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LassoCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cc674b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the Dataset\n",
    "\n",
    "df = pd.read_csv('./datasets/spotify_df.csv')\n",
    "final_df = pd.read_csv('./datasets/final_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c3117fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining X and y variables\n",
    "X = df.drop(columns=['popularity', 'chord'])\n",
    "y = df['popularity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44281fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebe6b657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('ss', StandardScaler()),\n",
       "                ('rf', RandomForestRegressor(random_state=42))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate Pipelines\n",
    "random_state = 42\n",
    "pipe_lr = Pipeline([('ss', StandardScaler()), #Linear Regression\n",
    "('lr', LinearRegression())])\n",
    "\n",
    "pipe_dt = Pipeline([('ss', StandardScaler()),\n",
    "('dt', DecisionTreeRegressor(random_state=random_state))]) #Decision Tree\n",
    "\n",
    "pipe_rf = Pipeline([('ss', StandardScaler()),\n",
    "                  ('rf', RandomForestRegressor(random_state=random_state))]) #Random Forest\n",
    "\n",
    "# Fitting Pipelines\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "pipe_dt.fit(X_train, y_train)\n",
    "pipe_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95ee1a4",
   "metadata": {},
   "source": [
    "#### Decision Tree Gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263d1036",
   "metadata": {},
   "source": [
    "I used the max_depth, min_sample_leaf, max_features, and max_leaf_nodes as starting parameters for the decision tree to prune the decision tree and help combat overfitting.\n",
    "- max_depth:\n",
    "I am tuning the max depth because without tuning it, the default is None. The deeper the tree, the more splits it will have, capturing more information about the data. But, with more splits, there is more likelihood of overfitting the model. Because my initial decision tree model was very overfit (Train: 1.0, Test: 0.38), I would want to try to minimize the overfitting while still capturing important info.\n",
    "\n",
    "- min_sample_leaf:\n",
    "The minimum number of samples that are required to be in the leaf node. Again, the more leaves, the more prone to overfitting.\n",
    "\n",
    "- max_features:\n",
    "The max features are the number of features to consider when looking for the best split. By using this hyperparameter, I will check to see if using a reduced number of features will help \"increase the stability of the tree and reduce variance and over-fitting.\"\n",
    "\n",
    "- max_leaf_nodes:\n",
    "The maximum number of leaf nodes the decision tree can have.\n",
    "\n",
    "[Source 1](https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680)\n",
    "[2](https://www.youtube.com/watch?time_continue=361&v=XABw4Y3GBR4&feature=emb_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b35612e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dt__max_depth': 6,\n",
       " 'dt__max_features': 'auto',\n",
       " 'dt__max_leaf_nodes': 20,\n",
       " 'dt__min_samples_leaf': 8}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Decision Tree Parameters\n",
    "# params_dt = {'dt__max_depth': [1,2,3,4,5,6,7,9,11,12],\n",
    "#              'dt__min_samples_leaf': [1,2,3,4,5,6,7,8,9,10],\n",
    "#              'dt__max_features': ['auto','log2','sqrt', None],\n",
    "#              'dt__max_leaf_nodes': [None,10,15,20,25,30,40,50,60,70,80,90]\n",
    "    \n",
    "# }\n",
    "\n",
    "# # Instantiate Gridsearch\n",
    "# gs_dt = GridSearchCV(pipe_dt, \n",
    "#                   param_grid=params_dt,\n",
    "#                   cv=5) \n",
    "# # Fit Gridsearch\n",
    "# gs_dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc59c5e5",
   "metadata": {},
   "source": [
    "#### Random Forest Gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41df8f77",
   "metadata": {},
   "source": [
    "The hyperparameters I used to help with overfitting are:\n",
    "- n_estimators:\n",
    "The number of trees in the model. The more trees may increase accuracy, but more trees = more computational time.\n",
    "\n",
    "- max_depth:\n",
    "The deeper the tree, the more splits it will have, capturing more information about the data. But, with more splits, there is more likelihood of overfitting the model.\n",
    "\n",
    "- min_samples_split:\n",
    "By increasing the number in this hyperparameter, it reduces the number of splits in the tree, helping overfitting.\n",
    "\n",
    "- mins_samples_leaf:\n",
    "By having a minimum number of samples required for a leaf node, this can help prevent \"the growth of the tree\", also preventing overfitting.\n",
    "\n",
    "- max_features:\n",
    "The max features are the number of features to consider when looking for the best split. By using this hyperparameter, I will check to see if using a reduced number of features will help \"increase the stability of the tree and reduce variance and over-fitting.\"\n",
    "\n",
    "[Source](https://rspiro9.github.io/hyperparameter_tuning_for_random_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a908b16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rf__max_depth': 20,\n",
       " 'rf__max_features': 'auto',\n",
       " 'rf__min_samples_leaf': 4,\n",
       " 'rf__min_samples_split': 2,\n",
       " 'rf__n_estimators': 400}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Random Forest Parameters\n",
    "# params_rf = {\n",
    "#     'rf__n_estimators': [200, 400, 600, 800, 1000],\n",
    "#     'rf__max_depth': [10, 20, 30, 40, 50, None],\n",
    "#     'rf__min_samples_split': [2, 5, 10],\n",
    "#     'rf__min_samples_leaf': [1,2,4],\n",
    "#     'rf__max_features': ['auto','log2','sqrt', None],\n",
    "# }\n",
    "\n",
    "# # Instantiate Gridsearch\n",
    "# gs_rf = GridSearchCV(pipe_rf,\n",
    "#                     param_grid=params_rf,\n",
    "#                     cv=3, n_jobs = -1)\n",
    "\n",
    "# # Fit Gridsearch\n",
    "# gs_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb6841e",
   "metadata": {},
   "source": [
    "#### ExtraTrees Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f2f1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ExtraTrees Parameters\n",
    "# params_xt = {\n",
    "#     'xt__n_estimators': [200, 400, 600, 800, 1000],\n",
    "#     'xt__max_depth': [10, 20, 30, 40, 50, None],\n",
    "#     'xt__min_samples_split': [2, 5, 10],\n",
    "#     'xt__min_samples_leaf': [1,2,4],\n",
    "#     'xt__max_features': ['auto','log2','sqrt', None],\n",
    "# }\n",
    "\n",
    "# # Instantiate Gridsearch\n",
    "# gs_xt = GridSearchCV(pipe_xt,\n",
    "#                   param_grid=params_xt,\n",
    "#                   cv=3, n_jobs = -1)\n",
    "\n",
    "# # Fit Gridsearch\n",
    "# gs_xt.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3902df",
   "metadata": {},
   "source": [
    "#### Gridsearch with Chosen Features from Polynomial and Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2f3b056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X\n",
    "X = final_df[['highest_charting_position', 'number_of_times_charted',\n",
    "       'streams',\n",
    "       'artist_followers', 'loudness', 'speechiness',\n",
    "       'tempo', 'valence',\n",
    "       'contemporary', 'edm', 'electropop', 'hip hop', 'house',\n",
    "       'indie', 'latin', 'other', 'pop', 'rap', 'reggaeton', 'rock', 'trap',\n",
    "       'danceability^2', 'energy loudness', 'chord', \n",
    "       'release_date_year']]\n",
    "# Dummify `chord`              \n",
    "X = pd.get_dummies(columns=['chord'], drop_first=True, data=X)\n",
    "\n",
    "# Drop chord features that have 0 coefficient\n",
    "X = X.drop(columns=['chord_A#/Bb', 'chord_C#/Db', 'chord_D', 'chord_E'])\n",
    "\n",
    "# Define Y\n",
    "y = final_df['popularity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b75946e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "774403fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('ss', StandardScaler()),\n",
       "                ('rf', RandomForestRegressor(random_state=42))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate Random Forest Pipeline\n",
    "random_state=42\n",
    "pipe_rf_coefs = Pipeline([('ss', StandardScaler()),\n",
    "                  ('rf', RandomForestRegressor(random_state=random_state))])\n",
    "\n",
    "# Fit Pipeline\n",
    "pipe_rf_coefs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8962c2e",
   "metadata": {},
   "source": [
    "The hyperparameters I used to help with overfitting are:\n",
    "- n_estimators:\n",
    "The number of trees in the model. The more trees may increase accuracy, but more trees = more computational time.\n",
    "\n",
    "- max_depth:\n",
    "The deeper the tree, the more splits it will have, capturing more information about the data. But, with more splits, there is more likelihood of overfitting the model.\n",
    "\n",
    "- min_samples_split:\n",
    "By increasing the number in this hyperparameter, it reduces the number of splits in the tree, helping overfitting.\n",
    "\n",
    "- mins_samples_leaf:\n",
    "By having a minimum number of samples required for a leaf node, this can help prevent \"the growth of the tree\", also preventing overfitting.\n",
    "\n",
    "- max_features:\n",
    "The max features are the number of features to consider when looking for the best split. By using this hyperparameter, I will check to see if using a reduced number of features will help \"increase the stability of the tree and reduce variance and over-fitting.\"\n",
    "\n",
    "[Source](https://rspiro9.github.io/hyperparameter_tuning_for_random_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcc2e092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rf__max_depth': 10,\n",
       " 'rf__max_features': 'auto',\n",
       " 'rf__min_samples_leaf': 1,\n",
       " 'rf__min_samples_split': 2,\n",
       " 'rf__n_estimators': 600}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Parameters\n",
    "# params_rf = {\n",
    "#     'rf__n_estimators': [200, 400, 600, 800, 1000],\n",
    "#     'rf__max_depth': [10, 20, 30, 40, 50, None],\n",
    "#     'rf__min_samples_split': [2, 5, 10],\n",
    "#     'rf__min_samples_leaf': [1,2,4],\n",
    "#     'rf__max_features': ['auto','log2','sqrt', None],\n",
    "# }\n",
    "\n",
    "# # Instantiate Random Forest Gridsearch\n",
    "# gs_rf = GridSearchCV(pipe_rf_coefs,\n",
    "#                     param_grid=params_rf,\n",
    "#                     cv=3, n_jobs = -1)\n",
    "\n",
    "# # Fit Gridsearch\n",
    "# gs_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7532d35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
